act.fct ="logistic",
linear.output = FALSE)
library("neuralnet")
NN <- neuralnet(CLASS~.,data=train,
hidden=c(10,10), rep = 5, err.fct = "ce",
linear.output = F, lifesign = "minimal", stepmax = 1000000,
threshold = 0.001)
NN <- neuralnet(CLASS~.,data=train,
hidden=c(10,10), rep = 5, err.fct = "ce",
linear.output = F, lifesign = "minimal",
stepmax = 1000000,
hidden=350,
threshold = 0.001)
NN <- neuralnet(CLASS~.,data=train, rep = 3, err.fct = "ce",
linear.output = F, lifesign = "minimal",
stepmax = 1000000,
hidden=350,
threshold = 0.001)
NN
plot(NN, rep="best")
train
formulaNN <- normal+suspect+pathologic ~ LB+AC+FM+UC+DL+DS+DP+ASTV+MSTV+ALTV+MLTV
Model <- train(form=FormulaNN,
data=train,
method="neuralnet",
### Parameters for layers
tuneGrid = expand.grid(.layer1=c(1:4), .layer2=c(0:4), .layer3=c(0)),
### Parameters for optmization
learningrate = 0.01,
threshold = 0.01,
stepmax = 50000
)
Model <- train(form=formulaNN,
data=train,
method="neuralnet",
### Parameters for layers
tuneGrid = expand.grid(.layer1=c(1:4), .layer2=c(0:4), .layer3=c(0)),
### Parameters for optmization
learningrate = 0.01,
threshold = 0.01,
stepmax = 50000
)
compute(NN,train)
compute(NN,train)$net.result
NN$result.matrix
pred1.NN <- compute(NN,train[,-12])
idx <- apply(pred1.NN$net.result, 1, which.max)
predicted1.NN <- c('normal','suspect','pathologic')[idx]
train1$CLASS
confusionMatrix(predicted1.NN,factor(train1$CLASS))
predicted1.NN
factor(train1$CLASS)
train1$CLASS
predicted1.NN
confusionMatrix(factor(predicted1.NN),factor(train1$CLASS))
pred2.NN <- compute(NN,test[,-12])
idx <- apply(pred2.NN$net.result, 1, which.max)
predicted2.NN <- c('normal','suspect','pathologic')[idx]
confusionMatrix(factor(predicted2.NN),factor(test1$CLASS))
pred1.NN <- compute(NN,train1[,-12])
idx <- apply(pred1.NN$net.result, 1, which.max)
predicted1.NN <- c('normal','suspect','pathologic')[idx]
pred2.NN <- compute(NN,test1[,-12])
idx <- apply(pred2.NN$net.result, 1, which.max)
predicted2.NN <- c('normal','suspect','pathologic')[idx]
confusionMatrix(factor(predicted1.NN),factor(train1$CLASS))
confusionMatrix(factor(predicted2.NN),factor(test2$CLASS))
confusionMatrix(factor(predicted2.NN),factor(test1$CLASS))
NN.results<-data.frame("MSE Training"=predicted1.NN,
"MSE Testing"=predicted2.NN)
rownames(NN.results)[1]<-'NN'
#plotting cool data frame
NN.results %>%
kbl() %>%
kable_classic(full_width = F, html_font = "Cambria")
library(devtools)
#import 'gar.fun' from Github
source_gist('6206737')
dim(train)[2]
pruned_model
vi_tree <- pruned_model$variable.importance
barplot(vi_tree, horiz = TRUE, las = 1)
barplot(vi_tree,
horiz = TRUE,
las = 1,
main="Variable Importance - TREE",
col="#69b3a2")
p1 <- vip(NN, type = "garson")
p2 <- vip(NN, type = "olden")
library(vip)
##
# Construct VIPs
install.packages('vip')
library('vip')
p1 <- vip(NN, type = "garson")
p2 <- vip(NN, type = "olden")
install.packages("NeuralNetTools")
##
# Construct VIPs
library(NeuralNetTools)
library(vip)
p1 <- vip(NN, type = "garson")
p2 <- vip(NN, type = "olden")
p2 <- vip(NN, type = "olden")
plot.nn(NN)
source_url('https://gist.githubusercontent.com/fawda123/7471137/raw/466c1474d0a505ff044412703516c34f1a4684a5/nnet_plot_update.r')
plot.nn(NN)
library(devtools)
plot.nnet(NN)
which.min(NN$result.matrix[1,])
plot(pmodel)
plot(train1,pch=16)
points(train1$FA, pred1.p, col = "blue", pch=4)
NN.plot <- neuralnet(CLASS~.,data=train, rep = min.rep, err.fct = "ce",
linear.output = F, lifesign = "minimal",
hidden=10,
threshold = 0.001)
min.rep<-which.min(NN$result.matrix[1,])
NN.plot <- neuralnet(CLASS~.,data=train, rep = min.rep, err.fct = "ce",
linear.output = F, lifesign = "minimal",
hidden=10,
threshold = 0.001)
plot.nnet(NN.plot)
NN.plot <- neuralnet(CLASS~.,data=train, rep = min.rep, err.fct = "ce",
linear.output = F, lifesign = "minimal",
stepmax = 1000000,
hidden=10,
threshold = 0.001)
plot.nnet(NN.plot)
plot(NN.plot)
plot(NN.plot)
plot(NN.plot,rep=best)
NN.plot <- neuralnet(CLASS~.,data=train, rep = min.rep,
linear.output = F,
hidden = c(4,2))
#k-means clustering
install.packages("ClusterR")
install.packages("cluster")
#k-means clustering
library(ClusterR)
library(cluster)
set.seed(123)
df.cluster <- kmeans(train1[,-12],
centers = 3,
nstart=20)
df.cluster$cluster
train1
confusionMatrix(train1$CLASS,df.cluster)
#Neural network
library("neuralnet")
##SVM polynomial
library("e1071")
# Prediction training and testing with MSE
library(caret) #confusion matrix
#plot
library('earth')
confusionMatrix(train1$CLASS,df.cluster)
train1$CLASS
df.cluster
confusionMatrix(train1$CLASS,df.cluster$cluster)
df.cluster$cluster
confusionMatrix(train1$CLASS,factor(df.cluster$cluster))
factor(df.cluster$cluster)
factor(train1$CLASS)
list(df.cluster$cluster)
factor(list(df.cluster$cluster))
df.cluster$cluster
levels(df.cluster$cluster)
data.frame(df.cluster$cluster)
data.frame(df.cluster$cluster)[,1]
confusionMatrix(train1$CLASS,data.frame(df.cluster$cluster)[,1])
train1$CLASS
factor(train1$CLASS)
factor(data.frame(df.cluster$cluster)[,1])
confusionMatrix(factor(train1$CLASS),factor(data.frame(df.cluster$cluster)[,1]))
factor(train1$CLASS)
train1$CLASS
confusionMatrix(train1$CLASS,factor(data.frame(df.cluster$cluster)[,1]))
set.seed(123)
df.cluster <- kmeans(train[,-12],
centers = 3,
nstart=20)
df.cluster$cluster
train
train$CLASS
confusionMatrix(factor(train$CLASS),factor(data.frame(df.cluster$cluster)[,1]))
as.numeric(train$CLASS)
test
numeric(train$CLASS)
as.numeric(train$CLASS)
as.numeric(train1$CLASS)
confusionMatrix(as.numeric(train1$CLASS),factor(data.frame(df.cluster$cluster)[,1]))
data.frame(df.cluster$cluster)[,1]
confusionMatrix(as.numeric(train1$CLASS),data.frame(df.cluster$cluster)[,1])
factor(as.numeric(train1$CLASS))
data.frame(df.cluster$cluster)[,1]
confusionMatrix(factor(as.numeric(train1$CLASS)),factor(data.frame(df.cluster$cluster)[,1]))
table(df.cluster$cluster,train$CLASS)
tot.withinss <- vector(mode="character", length=10)
for (i in 1:10){
irisCluster <- kmeans(train[,-12], center=i, nstart=20)
tot.withinss[i] <- irisCluster$tot.withinss
}
plot(1:10, tot.withinss, type="b", pch=19)
set.seed(123)
train1.labels <- train1$CLASS
table(train1.labels)
train1_1 <- train1[,-12]
train1_scale <- scale(train1_1)
train1_1 <- dist(train1_scale)
install.packages(factoextra)
install.packages('factoextra')
library(factoextra)
fviz_nbclust(train1_scale,kmeans,method='wss')+
labs(subtitle='Elbow method')
km.out <- kmeans(train1_scale,centers=3,nstart=100)
print(km.out)
km.cluster <- km.out$cluster
rownames(train1_scale) <- train1$CLASS
rownames(train1_scale) <- paste(train1$CLASS,
1:dim(train1)[1],sep='_')
fviz_cluster(list(data=train1_scale,cluster=km.cluster))
table(km.cluster,train1$CLASS)
#we might use 3 in the first case
km3.out <- kmeans(train1_scale,centers=3,nstart=100)
print(km3.out)
#visualization
km3.cluster <- km3.out$cluster
rownames(train1_scale) <- paste(train1$CLASS,
1:dim(train1)[1],sep='_')
fviz_cluster(list(data=train1_scale,cluster=km3.cluster))
table(km3.cluster,train1$CLASS)
fviz_nbclust(train1_scale,kmeans,method='wss')+
labs(subtitle='Elbow method')
k.decision<-fviz_nbclust(train1_scale,kmeans,method='wss')+
labs(subtitle='Elbow method')
km6.out <- kmeans(train1_scale,centers=6,nstart=100)
print(km6.out)
#visualization
km6.cluster <- km6.out$cluster
rownames(train1_scale) <- paste(train1$CLASS,
1:dim(train1)[1],sep='_')
fviz_cluster(list(data=train1_scale,cluster=km6.cluster))
table(km6.cluster,train1$CLASS)
fh
library(pastecs)
sum.df<-stat.desc(df) #
round(sum.df,3)
library(RColorBrewer)
coor.table <- df %>%
ggplot(aes(ASTV,MSTV)) +
geom_point(aes(shape=18,size = 0.7,color=factor(CLASS))) +
scale_shape_identity()+
theme(legend.position = "none") +
labs(title = "", caption = "Fig.4")+
theme_minimal() +
scale_color_brewer(palette = "Dark2")
library(dplyr)
coor.table <- df %>%
ggplot(aes(ASTV,MSTV)) +
geom_point(aes(shape=18,size = 0.7,color=factor(CLASS))) +
scale_shape_identity()+
theme(legend.position = "none") +
labs(title = "", caption = "Fig.4")+
theme_minimal() +
scale_color_brewer(palette = "Dark2")
coor.table
library(GGally)
df1<-df
df1$CLASS<-as.factor(df$CLASS)
ggpairs(df1, ggplot2::aes(colour = CLASS, alpha = 0.4))
plotmo(MLN)
MLN<-multinom(CLASS~.,data=train)
library(nnet)
set.seed(123)
MLN<-multinom(CLASS~.,data=train)
plotmo(MLN)
library(plotmo)
plotmo(MLN)
plotmo(bst.tree.df, pmethod="partdep",all1=TRUE)
t(z)
MLN.results %>%
kbl() %>%
kable_classic(full_width = F, html_font = "Cambria")
library(kableExtra)
MLN.results %>%
kbl() %>%
kable_classic(full_width = F, html_font = "Cambria")
confusionMatrix(factor(test.pred.MLN),factor(list(test[,12])[[1]]$CLASS))
test.pred.MLN
test[,12]
list(test[,12])
factor(list(test[,12]))
list(test[,12])
list(test[,12])[[1]]
factor(list(test[,12])[[1]])
test.pred.MLN
confusionMatrix(factor(test.pred.MLN),factor(list(test[,12])[[1]]))
confusionMatrix(factor(train.pred.MLN),factor(list(train[,12])[[1]]))
library(caret)
confusionMatrix(factor(test.pred.MLN),factor(list(test[,12])[[1]]))
confusionMatrix(factor(train.pred.MLN),factor(list(train[,12])[[1]]))
KNN3.results %>%
kbl() %>%
kable_classic(full_width = F, html_font = "Cambria")
library(kableExtra)
library(dplyr)
KNN3.results %>%
kbl() %>%
kable_classic(full_width = F, html_font = "Cambria")
cftr.matrix.KNN<-confusionMatrix(factor(tr.KNN3),factor(train[,12]))
cftr.matrix.KNN
cftst.matrix.KNN
rpart.plot(tree.df,type=1,sub='')
library(MASS)
library(rpart)
library(rpart.plot)
library(rattle)
rpart.plot(tree.df,type=1,sub='')
rpart.plot(tree.df,type=1,sub='',cex=0.4)
rpart.plot(tree.df,type=1,sub='',cex=0.6)
print(tree.df) #tree on text
summary(tree.df) #results over the entire process
ggtree(tree.df)
#plot
library(ggplot2)
ggtree(tree.df)
library(ape)
library(ggtree)
install.packages('ape')
install.packages('ggtree')
library(ape)
library(ggtree)
install.packages('BiocManager')
library(BiocManager)
BiocManager::install("ggtree")
ggtree(tree.df)
library(ggtree)
printcp(tree.df)
plotcp(tree.df)
plotmo(bst.tree.df, pmethod="partdep",all1=TRUE)
library(plotmo)
plotmo(bst.tree.df, pmethod="partdep",all1=TRUE)
TREE.results %>%
kbl() %>%
kable_classic(full_width = F, html_font = "Cambria")
cm.tst.tree
cm.tr.tree
barplot(vi_tree,
horiz = TRUE,
las = 1,
main="Variable Importance - TREE",
col="#69b3a2")
confusionMatrix(pred1.p,factor(train1$CLASS))
table(km6.cluster,train1$CLASS)
confusionMatrix(factor(predicted2.NN),factor(test1$CLASS))
confusionMatrix(factor(predicted1.NN),factor(train1$CLASS))
confusionMatrix(pred2.p,factor(test1$CLASS))
confusionMatrix(pred1.p,factor(train1$CLASS))
LDA <- lda(CLASS~., data=train)
train
LDA <- lda(train[,-12], grouping=train[,12])
train1
train2<-train1
train2$CLASS <- as.numeric(train2$CLASS)
train2
LDA <- lda(train1[,-12], grouping=train1[,12])
LDA <- lda(train2[,-12], grouping=train2[,12])
train2
train2 <- data.frame(train2)
LDA <- lda(train2[,-12], grouping=train2[,12])
library(MASS)
LDA <- lda(train2[,-12], grouping=train2[,12])
LDA <- lda(CLASS~.,train1)
LDA
LDA <- train(CLASS~.,train1,method='lda')
LDA
LDA <- qda(CLASS~.,train1)
LDA <- qda(CLASS~.,train2)
LDA <- qda(CLASS~.,train1)
wwdf
wwdf
summary(MLN)
summary(MLN)
t(z)
UP.MLN1 <- update(MLN, ~.-LB-MLTV)
library(nnet)
UP.MLN1 <- update(MLN, ~.-LB-MLTV)
summary(UP.MLN1)
summary(MLN)
z1 = summary(UP.MLN1)$coefficients/summary(UP.MLN1)$standard.errors
z1 = (1 - pnorm(abs(z1), 0, 1)) * 2
t(z1)
UP.MLN1.2 <- update(MLN, ~.-LB)
summary(UP.MLN1.2)
z1.2 = summary(UP.MLN1.2)$coefficients/summary(UP.MLN1.2)$standard.errors
z1.2 = (1 - pnorm(abs(z1.2), 0, 1)) * 2
t(z1.2)
UP.MLN1.3 <- update(MLN, ~.-MLTV)
summary(UP.MLN1.3)
z1.3 = summary(UP.MLN1.3)$coefficients/summary(UP.MLN1.3)$standard.errors
z1.3 = (1 - pnorm(abs(z1.3), 0, 1)) * 2
t(z1.3)
summary(MLN)
t(z)
UP.MLN2 <- update(MLN, ~.-MSTV-MLTV)
summary(UP.MLN2)
z2 = summary(UP.MLN2)$coefficients/summary(UP.MLN2)$standard.errors
z2 = (1 - pnorm(abs(z2), 0, 1)) * 2
t(z2)
UP.MLN2.1 <- update(MLN, ~.-MLTV)
summary(UP.MLN2.1)
summary(MLN)
summary(UP.MLN2.1)
#p-values
z2.1 = summary(UP.MLN2.1)$coefficients/summary(UP.MLN2.1)$standard.errors
z2.1 = (1 - pnorm(abs(z2.1), 0, 1)) * 2
t(z2.1)
t(z1)
LR.BCK.STEP <- step(MLN,direction = "backward",k=log(nrow(train)))
summary(LR.BCK.STEP)#the best model with lowest AIC
summary(MLN)
LR.BCK.STEP <- step(MLN,direction = "both",k=log(nrow(train)))
summary(LR.BCK.STEP)#the best model with lowest AIC
LR.BCK.STEP <- step(MLN,direction = "both",k=2)
summary(LR.BCK.STEP)#the best model with lowest AIC
LR.BCK.STEP <- step(MLN,direction = "forward",k=2)
summary(LR.BCK.STEP)#the best model with lowest AIC
LR.BCK.STEP <- step(MLN,direction = "backward",k=2)
summary(LR.BCK.STEP)#the best model with lowest AIC
ab <- multinom(CLASS ~ LB + AC + FM + UC + DL + DP + ASTV +
MSTV + ALTV + MLTV, data = train)
summary(ab)
t(z1)
summary(MLN)
summary(LR.BCK.STEP)#the best model with lowest AIC
final_MLN <- multinom(CLASS ~ LB + AC + FM + UC + DL + DP + ASTV +
MSTV + ALTV + MLTV, data = train)
summary(final_MLN)
# Prediction training and testing with MSE
train.pred.matrix <- data.frame(round((final_MLN$fitted.values),2))
position.train.pred.matrix <- max.col(train.pred.matrix,ties.method="first")
train.pred.MLN <- colnames(train.pred.matrix)[position.train.pred.matrix]
MSE.train.pred.MLN <- mean(train[,12]!=train.pred.MLN)
test.pred.MLN <- predict(final_MLN,newdata=test,"class")
MSE.test.pred.MLN <- mean(factor(list(test[,12])[[1]]$CLASS)!= factor(test.pred.MLN))
# Prediction training and testing with MSE
train.pred.matrix <- data.frame(round((final_MLN$fitted.values),2))
position.train.pred.matrix <- max.col(train.pred.matrix,ties.method="first")
train.pred.MLN <- colnames(train.pred.matrix)[position.train.pred.matrix]
MSE.train.pred.MLN <- mean(train[,12]!=train.pred.MLN)
test.pred.MLN <- predict(final_MLN,newdata=test,"class")
MSE.test.pred.MLN <- mean(factor(list(test[,12])[[1]])!= factor(test.pred.MLN))
#results
library(kableExtra)
MLN.results<-data.frame("MSE Training"=MSE.train.pred.MLN,
"MSE Testing"=MSE.test.pred.MLN)
rownames(MLN.results)[1]<-'MLN'
#plotting cool data frame
MLN.results %>%
kbl() %>%
kable_classic(full_width = F, html_font = "Cambria")
library(caret)
confusionMatrix(factor(test.pred.MLN),factor(list(test[,12])[[1]]))
confusionMatrix(factor(train.pred.MLN),factor(list(train[,12])[[1]]))
table(km6.cluster,train1$CLASS)
confusionMatrix(factor(predicted2.NN),factor(test1$CLASS))
confusionMatrix(factor(predicted1.NN),factor(train1$CLASS))
LDA <- qda(CLASS~.,train1)
confusionMatrix(pred1.p,factor(train1$CLASS))
confusionMatrix(factor(predicted2.NN),factor(test1$CLASS))
seq(0,1,0.1)
2^(2:7)
summary(pmodel)
best_model_psvm
plot(pmodel)
summary(best_model_psvm)
summary(svm_pmodel)
#plotting cool data frame
SVM.results %>%
kbl() %>%
kable_classic(full_width = F, html_font = "Cambria")
MLN.results %>%
kbl() %>%
kable_classic(full_width = F, html_font = "Cambria")
KNN3.results %>%
kbl() %>%
kable_classic(full_width = F, html_font = "Cambria")
TREE.results %>%
kbl() %>%
kable_classic(full_width = F, html_font = "Cambria")
SVM.results %>%
kbl() %>%
kable_classic(full_width = F, html_font = "Cambria")
confusionMatrix(pred1.p,factor(train1$CLASS))
confusionMatrix(pred2.p,factor(test1$CLASS))
nrow(train[train$CLASS == 'normal'])
nrow(train[,train$CLASS == 'normal'])
nrow(train[train$CLASS == 'normal',])
1330-1317
nrow(test[test$CLASS == 'normal',])
325-311
nrow(train[train$CLASS == 'suspect',])
236-128
nrow(test[test$CLASS == 'suspect',])
38-59
nrow(train[train$CLASS == 'pathologic',])
195-134
nrow(test[test$CLASS == 'pathologic',])
39-42
confusionMatrix(pred1.p,factor(train1$CLASS))
confusionMatrix(pred2.p,factor(test1$CLASS))
